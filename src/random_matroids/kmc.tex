\section{Knuth's matroid construction (KMC)}
\pr{Knuth-Matroid} (given in Algorithm~\ref{alg:knuth}) accepts the ground set $E$ and a list of enlargements $\mathrm{X}$, and produces the matroid $\mathfrak{M} = (E, \mathcal{F})$, such that for each set $X \in \mathrm{X}[r]$, we have $X \in \mathcal{F}, \text{ rank}(X) = r$. The algorithm outputs the tuple $(E, \mathrm{F})$, where $\mathrm{F} = [F_0, \ldots, F_r]$, $r$ being the final rank of $\mathfrak{M}$ and $F_i$ the family of closed sets of rank $i$. In the paper, Knuth shows that $\bigcup_{i=0}^r \mathrm{F}[r] = \mathcal{F}$, and so the algorithm produces a valid matroid represented by its closed sets.

The \textit{covers} of a closed set $A$ of rank $r$ are the sets obtained by adding one more element from $E$ to $A$. The algorithm proceeds in a bottom-up manner, starting with the single closed set of rank 0 (the empty set) and for each rank $r+1$ adds the covers of the closed sets of rank $r$. The covers are generated with the helper method \pr{Generate-Covers}(\mathrm{F}, r, E).

\begin{tcolorbox}[pseudo/boxruled]
  \begin{pseudo}*
    \hd{Generate-Covers}(\mathrm{F}, r, E) \\
    return $\{ A \cup \{a\} : A \in \mathrm{F}[r], a \in E \setminus A \}$
  \end{pseudo}
\end{tcolorbox}

Given no enlargements ($\mathrm{X} = []$), the resulting matroid is the free matroid over $E$. Arbitrary matroids can be generated by supplying different lists $\mathrm{X}$. When enlarging, the sets in $\mathrm{X}[r+1]$ are simply added to $\mathrm{F}[r+1]$.

\pr{Superpose!}(\mathrm{F}[r+1], F[r]) ensures that the newly enlarged family of closed sets of rank $r+1$ is valid. If $F_{r+1}$ contains two sets $A,B$ whose intersection $A \cap B \not \subseteq C$ for any $C \in F_{r}$, replace $A,B$ with $A \cup B$. Repeat until no two sets exist in $F_{r+1}$ whose intersection is not contained within some set $C \in F_{r}$.

\begin{tcolorbox}[pseudo/boxruled, float*=ht!]
  \begin{pseudo}[kw, indent-mark]*
    \hd{Superpose!}({F_{r+1},F_r}) \\
    for $A \in F_{r+1}$ \\+
    for $B \in F_{r+1}$ \\+
    \id{flag} $\leftarrow$ \cn{true} \\
    for $C \in F_r$ \\+
    if $A \cap B \subseteq C$ \\+
    \id{flag} $\leftarrow$ \cn{false} \\--
    \\
    if \id{flag} = \cn{true} \\+
    $F_{r+1} \leftarrow F_{r+1} \setminus \{A, B \}$ \\
    $F_{r+1} \leftarrow F_{r+1} \cup \{A \cup B \}$
  \end{pseudo}
\end{tcolorbox}

\begin{algorithm}[float*=ht!]{\pr{Knuth-Matroid}(E, \mathrm{X})}{knuth}

  \textbf{Input:}     \tab The ground set of elements $E$, and a list of enlargements $\mathrm{X}$.

  \textbf{Output:}    \tab The list of closed sets of the resulting matroid grouped by rank, \\
  \mbox{}\tab $\mathrm{F} = [F_0, \ldots, F_r]$, where $F_i$ is the set of closed sets of rank $i$.

  \begin{pseudo}[kw, label=\small\arabic*, indent-mark, line-height=1.2]
    $r = 0, \mathrm{F} = [\{ \emptyset \}]$ \\
    while \cn{true}  \\+
    $\pr{Push!}(\mathrm{F}, \pr{Generate-Covers}(\mathrm{F}, r, E))$ \\
    $\mathrm{F}[r+1] = \mathrm{F}[r+1] \cup \mathrm{X}[r+1]$ \\
    \pr{Superpose!}(\mathrm{F}[r+1], \mathrm{F}[r]) \\

    if $E \not \in F[r+1]$ \\+
    $r \leftarrow r+1$ \\-
    else \\+
    return $(E, \mathrm{F})$

  \end{pseudo}

\end{algorithm}


\subsection{Randomized KMC}
In the randomized version of \pr{Knuth-Matroid}, we generate matroids by applying a supplied number of random coarsening steps, instead of enlarging with supplied sets. This is done by applying \pr{Superpose!} immediately after adding the covers, then choosing a random member $A$ of $\mathrm{F}[r+1]$ and a random element $a \in E \setminus A$, replacing $A$ with $A \cup \{a\}$ and finally reapplying \pr{Superpose!}. The parameter $p = (p_1, p_2, \ldots)$ gives the number of such coarsening steps to be applied at each iteration of the algorithm.

The pseudocode given up to this point corresponds closely to the initial Julia implementation, which can be found in Appendix~\ref{apx:randkmcv1}. It should already be clear that this brute force implementation leads to poor performance -- for instance, the \pr{Superpose!} method uses a triply nested for loop, which should be a candidate for significant improvement if possible. Section~\ref{sec:improving-performance} describes the engineering work done to create a more performant implementation.

\begin{algorithm}[float*=ht!]{\pr{Randomized-Knuth-Matroid}(E, p)}{rkmc}

  \textbf{Input:}     \tab The ground set of elements $E$, and a list $p = [p_1, p_2, ...]$, where \\
  \mbox{}\tab $p_r$ is the number of coarsening steps to apply at rank $r$ in the \\
  \mbox{}\tab construction.

  \textbf{Output:}    \tab The list of closed sets of the resulting matroid grouped by rank, \\
  \mbox{}\tab $\mathrm{F} = [F_0, \ldots, F_r]$, where $F_i$ is the set of closed sets of rank $i$.

  \begin{pseudo}[label=\small\arabic*, indent-mark, line-height=1.2]
    $r = 0, \mathrm{F} = [\{ \emptyset \}]$ \\
    \kw{while} \cn{true}  \\+
      $\pr{Push!}(\mathrm{F}, \pr{Generate-Covers}(\mathrm{F}, r, E))$ \\
      \pr{Superpose!}(\mathrm{F}[r+1], \mathrm{F}[r]) \\
      
      \kw{if} $E \in \mathrm{F}[r+1]$ \kw{return} $(E, \mathrm{F})$ \\
      
      \kw{while} $p[r] > 0$ \\+
        $A \leftarrow$ a random set in $\mathrm{F}[r+1]$ \\
        $a \leftarrow$ a random element in $E \setminus A$ \\
        \kw{replace} $A$ with $A \cup \{a\}$ in $\mathrm{F}[r+1]$ \\
        \pr{Superpose!}(\mathrm{F}[r+1], \mathrm{F}[r]) \\

        \kw{if} $E \in \mathrm{F}[r+1]$ \kw{return} $(E, \mathrm{F})$ \\

        $p[r] = p[r] - 1$ \\-
      $r = r + 1$

  \end{pseudo}

\end{algorithm}


\subsection{Improving performance}
\label{sec:improving-performance}
When recreating Knuth's table of observed mean values for the randomly generated matroids, some of the latter configurations of $n$ and $(p_1, p_2, \ldots)$ was unworkably slow, presumably due to my naïve implementation of the algorithm. Table~\ref{tab:perf_v1} shows the performance of this first implementation.

\begin{table*}[ht!]
  \centering
  \caption{Performance of $\texttt{random\_kmc\_v1}$.}
  \label{tab:perf_v1}
  \begin{threeparttable}
    \begin{tabular}{llllllllll}
      \toprule
      $n$ & $(p_1, p_2, \ldots)$ & Trials & Time  & GC Time & Bytes allocated \\
      \midrule
        10 & (0, 6, 0)    & 100 & 0.0689663   & 0.0106786 & 147.237 MiB \\
        10 & (0, 5, 1)    & 100 & 0.1197194   & 0.0170734 & 251.144 MiB \\
        10 & (0, 5, 2)    & 100 & 0.0931822   & 0.0144022 & 203.831 MiB \\
        10 & (0, 6, 1)    & 100 & 0.0597314   & 0.0094902 & 132.460 MiB \\
        10 & (0, 4, 2)    & 100 & 0.1924601   & 0.0284532 & 406.131 MiB \\
        10 & (0, 3, 3)    & 100 & 0.3196838   & 0.0463972 & 678.206 MiB \\
        10 & (0, 0, 6)    & 100 & 1.1420602   & 0.1671325 & 2.356 GiB   \\
        10 & (0, 1, 1, 1) & 100 & 2.9283978   & 0.3569357 & 5.250 GiB   \\
        13 & (0, 6, 0)    & 10  & 104.0171128 & 9.9214449 & 161.523 GiB \\
        13 & (0, 6, 2)    & 10  & 11.4881308  & 1.3777947 & 20.888 GiB  \\
        16 & (6, 0, 0)    & 1   & -           & -         & -           \\
      \bottomrule
    \end{tabular}
  \end{threeparttable}
\end{table*}

The performance was measured using Julia's \jlinl{@timed}\footnote{\href{https://docs.julialang.org/en/v1/base/base/\#Base.@timed}{https://docs.julialang.org/en/v1/base/base/\#Base.@timed}} macro, which returns the time it takes to execute a function call, how much of that time was spent in garbage collection and the number of bytes allocated. As is evident from the data, larger matroids are computationally quite demanding to compute with the current approach, and the time and space requirements scales exponentially with $n$. Can we do better? As it turns out, we can; after the improvements outlined in this section, we will be able to generate matroids over universes as large as $n=128$ in a manner of seconds and megabytes.

\subsubsection{Representing sets as binary numbers}
The first improvement we will attempt is to represent our closed sets using one of Julia's \jlinl{Integer} types of bit width at least $n$, instead of as a \jlinl{Set}\footnote{\href{https://docs.julialang.org/en/v1/base/collections/\#Base.Set}{https://docs.julialang.org/en/v1/base/collections/\#Base.Set}} of elements of $E$. Appendix~\ref{apx:code} contains all the code referenced in this chapter; the Julia implementation at this point can be found in \ref{apx:randkmcv2}. 

\skelpar{Skrive mer om hvordan Set\{Set\{Integer\}\} lagres i minnet og fordelene med å gå over til Set\{Integer\}.}

The idea is to define a family of closed sets of the same rank as \jlinl{Set\{UInt16\}}. Using \jlinl{UInt16} we can support ground sets of size up to 16. Each 16-bit number represents a set in the family. For example, the set $\{ 2,5,7 \}$ is represented by $$164 = 0\rm{x}00\rm{a}4 = 0\rm{b}0000000010100100 = 2^7+2^5+2^2.$$ At either end we have $\emptyset \equiv 0\rm{x}0000$ and $E \equiv 0\rm{xffff}$ (if $n = 16$). The elementary set operations we will need have simple implementations using bitwise operations.

\begin{table}[!ht]
  % \caption{Set operations and their equivalent bitwise operations}
  \centering
  \begin{tabular}{|l|l|}
  \hline
      Set operation & Bitwise operation \\\hline
      $A \cap B$      & $A$ AND $B$ \\\hline
      $A \cup B$      & $A$ OR $B$ \\\hline
      $A \setminus B$ & $A$ AND NOT $B$ \\\hline
      $A \subseteq B$ & $A$ AND $B$ = $A$ \\\hline
  \end{tabular}
\end{table}

We can now describe the bitwise versions of the required methods. The bitwise implementation of \pr{Generate-Covers} finds all elements in $E \setminus A$ by finding each value $0\leq i< n$ for which \jlinl{A & 1 << i === 0}, meaning that the set represented by \jlinl{1 << i} is not a subset of A. The bitwise implementation of \pr{Superpose!} is unchanged apart from using the bitwise set operations described above.

\begin{table*}[ht!]
  \centering
  \caption{Performance of $\texttt{random\_kmc\_v2}$.}
  \label{tab:perf_v2}
  \begin{threeparttable}
    \begin{tabular}{llllllllll}
      \toprule
      $n$ & $(p_1, p_2, \ldots)$ & Trials & Time  & GC Time & Bytes allocated \\
      \midrule
      10 & [0, 6, 0] & 100 & 0.0010723 & 0.0001252 & 1.998 MiB \\ 
      10 & [0, 5, 1] & 100 & 0.0017543 & 0.0001431 & 3.074 MiB \\ 
      10 & [0, 5, 2] & 100 & 0.0008836 & 0.0001075 & 2.072 MiB \\ 
      10 & [0, 6, 1] & 100 & 0.0007294 & 6.73e-5 & 1.700 MiB \\ 
      10 & [0, 4, 2] & 100 & 0.0020909 & 0.0001558 & 3.889 MiB \\ 
      10 & [0, 3, 3] & 100 & 0.0024636 & 0.0002139 & 4.530 MiB \\ 
      10 & [0, 0, 6] & 100 & 0.007082 & 0.0004801 & 9.314 MiB \\ 
      10 & [0, 1, 1, 1] & 100 & 0.0132477 & 0.0008307 & 17.806 MiB \\ 
      13 & [0, 6, 0] & 10 & 0.042543 & 0.0014988 & 31.964 MiB \\ 
      13 & [0, 6, 2] & 10 & 0.0183313 & 0.0012176 & 21.062 MiB \\ 
      16 & [0, 6, 0] & 10 & 1.2102877 & 0.0146129 & 450.052 MiB \\ 
      \bottomrule
    \end{tabular}
  \end{threeparttable}
\end{table*}

The performance of \mono{random\_kmc\_v2} is shown in Table~\ref{tab:perf_v2}. It is clear that representing closed sets using binary numbers represents a substantial improvement -- we are looking at performance increases of 100x-1000x across the board. Great stuff!


\subsubsection{Sorted superpose}
Can we improve the running time of the algorithm further? It is clear that \pr{Superpose!} takes up a large portion of the compute time. In the worst case, when no enlargements have been made, $F_{r+1}$ is the set of all $r+1$-sized subsets of $E$, $|F_{r+1}| = {\binom{n}{r+1}}$. Comparing each $A,B \in F_{r+1}$ with each $C \in F_r$ in a triply nested for loop requires $\mathcal{O}({\binom{n}{r+1}}^2{\binom{n}{r}})$ operations. In the worst case, no enlargements are made at all, and we build the free matroid in $\mathcal{O}(2^{3n})$ time (considering only the superpose step).

After larger closed sets have been added to $\mathrm{F}[r+1]$, \pr{Superpose!} will cause sets to merge, so that only maximal dependent sets remain. Some sets will even simply disappear. In the case where $X=\{1,2\}$ was added by \pr{Generate-Covers}, and the $Y=\{1,2,3\}$ was added manually as an enlargement, the smaller set will be fully subsumed in the bigger set, as $\{1,2\}\cap\{1,2,3\}=\{1,2\}$ (which is not a subset of any set in $\mathrm{F}[r]$) and $\{1,2\}\cup\{1,2,3\}=\{1,2,3\}$. In this situation, $Y$ would ``eat'' the covers $\{1,3\}$ and $\{2,3\}$ as well. This fact is reflected in the performance data -- compare the memory allocation differences between the 10-element matroid with $p=[0,0,6]$ and the one with $p=[0,6,0]$ in any of the performance tables in this section. Making enlargements at earlier ranks result in smaller matroids as more sets get absorbed.

\begin{jllisting}
function sorted_bitwise_superpose!(F, F_prev)
  As = sort!(collect(F), by = s -> length(bits_to_set(s)))
  while length(As) !== 0
    A = popfirst!(As)

    for B in setdiff(F, A)
      if should_merge(A, B, F_prev)
        insert!(As, 1, A | B)
        setdiff!(F, [A, B])
        push!(F, A | B)
        break
      end
    end
  end

  return F
end
\end{jllisting}

Since the larger sets will absorb so many of the smaller sets (around $\binom{p}{r+1}$, where $p$ is the size of the larger set and $r+1$ is the size of the smallest sets allowed to be added in a given iteration), might it be an idea to perform the superpose operation in descending order based on the size of the sets? This should result in fewer calls to \pr{Superpose!}, as the bigger sets will remove the smaller sets that fully overlap with them in the early iterations, however, the repeated sorting of the sets might negate this performance gain. This is the idea behind \jlinl{sorted_bitwise_superpose!}, which was used in \jlinl{random_kmc_v3}. The full code can be found in Appendix~\ref{apx:randkmcv2}.

Unfortunately, as Table~\ref{tab:perf_v3} shows, this implementation is a few times slower and more space demanding than the previous implementation. This is might be due to the fact that an ordered list is more space inefficient than the hashmap-based \jlinl{Set}.

\begin{table*}[ht!]
  \centering
  \caption{Performance of $\texttt{random\_kmc\_v3}$.}
  \label{tab:perf_v3}
  \begin{threeparttable}
    \begin{tabular}{llllllllll}
      \toprule
      $n$ & $(p_1, p_2, \ldots)$ & Trials & Time  & GC Time & Bytes allocated \\
      \midrule
      10 & [0, 6, 0] & 100 & 0.0023382 & 0.0001494 & 4.042 MiB \\
      10 & [0, 5, 1] & 100 & 0.001853 & 0.0001433 & 4.383 MiB \\
      10 & [0, 5, 2] & 100 & 0.0017845 & 0.0001341 & 4.043 MiB \\
      10 & [0, 6, 1] & 100 & 0.0015145 & 0.0001117 & 3.397 MiB \\
      10 & [0, 4, 2] & 100 & 0.0030704 & 0.0002125 & 6.385 MiB \\
      10 & [0, 3, 3] & 100 & 0.0037838 & 0.0002514 & 7.018 MiB \\
      10 & [0, 0, 6] & 100 & 0.008903 & 0.000557 & 14.159 MiB \\
      10 & [0, 1, 1, 1] & 100 & 0.0142828 & 0.0008823 & 21.838 MiB \\
      13 & [0, 6, 0] & 10 & 0.0627633 & 0.002094 & 51.492 MiB \\
      13 & [0, 6, 2] & 10 & 0.0106478 & 0.0007704 & 20.774 MiB \\
      16 & [0, 6, 0] & 10 & 0.6070136 & 0.0095656 & 310.183 MiB \\
      \bottomrule
    \end{tabular}
  \end{threeparttable}
\end{table*}


\subsubsection{Iterative superpose}
The worst-case $\mathcal{O}({\binom{n}{r+1}}^2{\binom{n}{r}})$ runtime of \pr{Superpose!} at step $r$ is due to the fact that it takes in $\mathrm{F}$ after all covers and enlargements have been indiscriminately added to $\mathrm{F}[r+1]$ and then loops through to perform the superposition. Might there be something to gain by inserting new closed sets into the current family one at a time, and superposing on the fly?
\pagebreak
\begin{jllisting}
  # Superpose (random_kmc_v4)
  push!(F, Set()) # Add F[r+1].
  while length(to_insert) > 0
    A = pop!(to_insert)
    push!(F[r+1], A)

    for B in setdiff(F[r+1], A)
      if should_merge(A, B, F[r])
        push!(to_insert, A | B)
        setdiff!(F[r+1], [A, B])
        push!(F[r+1], A | B)
      end
    end
  end
\end{jllisting}

In \jlinl{random_kmc_v4}, the full code of which can be found in Appendix~\ref{apx:randkmcv4}, the covers and enlargements are not added directly to $\mathrm{F}[r+1]$, but to a temporary array \jlinl{to_insert}. Each set $A$ is then popped from \jlinl{to_insert} one at a time, added to $\mathrm{F}[r+1]$ and compared with the other sets $B \in \mathrm{F}[r+1] \setminus \{A\}$ and $C \in \mathrm{F}[r]$ in the usual \pr{Superpose!} manner. This results in fewer comparisons, as each set is only compared with the sets added before it; the first set is compared with no other sets, the second set with one other and the sets in $\mathrm{F}[r]$, and so on. The number of such comparisons is therefore given by the triangular number $T_{\binom{n}{r+1}}$, and so we should have roughly halved the runtime at step $r$. It is worth noting that this implementation of \pr{Superpose!} uses a subroutine \jlinl{should_merge} that returns early when it finds one set $C \in \mathrm{F}[r]$ such that $C \supseteq A \cap B$, so in practice it usually does not require $\binom{n}{r}$ comparisons in the innermost loop.

Table~\ref{tab:perf_v4} shows that the iterative superpose was a meaningful improvement. For most input configurations, it is a few times faster and a few times less space demanding than \jlinl{random_kmc_v2}.


\begin{table*}[ht!]
  \centering
  \caption{Performance of $\texttt{random\_kmc\_v4}$.}
  \label{tab:perf_v4}
  \begin{threeparttable}
    \begin{tabular}{llllllllll}
      \toprule
      $n$ & $(p_1, p_2, \ldots)$ & Trials & Time  & GC Time & Bytes allocated \\
      \midrule
      10 & [0, 6, 0]    & 100 & 0.0014585  & 3.94e-5   & 724.635 KiB \\ 
      10 & [0, 5, 1]    & 100 & 0.0007192  & 9.39e-5   & 659.729 KiB \\ 
      10 & [0, 5, 2]    & 100 & 0.0005943  & 3.53e-5   & 617.668 KiB \\ 
      10 & [0, 6, 1]    & 100 & 0.0003502  & 2.88e-5   & 408.666 KiB \\ 
      10 & [0, 4, 2]    & 100 & 0.001013   & 5.36e-5   & 887.618 KiB \\ 
      10 & [0, 3, 3]    & 100 & 0.0011847  & 5.03e-5   & 1.003 MiB   \\ 
      10 & [0, 0, 6]    & 100 & 0.0015756  & 9.7e-5    & 1.066 MiB   \\ 
      10 & [0, 1, 1, 1] & 100 & 0.0046692  & 0.0001385 & 2.455 MiB   \\ 
      13 & [0, 6, 0]    & 10  & 0.0118201  & 0.0005486 & 6.289 MiB   \\ 
      13 & [0, 6, 2]    & 10  & 0.0075668  & 0.0002458 & 4.666 MiB   \\ 
      16 & [0, 6, 0]    & 10  & 0.2819294  & 0.0040792 & 81.317 MiB  \\ 
      16 & [0, 6, 1]    & 10  & 0.8268207  & 0.0070206 & 154.451 MiB \\ 
      16 & [0, 0, 6]    & 10  & 95.1959596 & 0.0290183 & 553.597 MiB \\ 
      \bottomrule
    \end{tabular}
  \end{threeparttable}
\end{table*}

\subsubsection{Rank table}
While \pr{Superpose!} is getting more efficient, it is still performing the same comparisons over and over again. Let's consider what we are really trying to achieve with this function, to see if we can't find a smarter way to go about it.

After adding the closed sets for a rank, \pr{Superpose!} is run to maintain the closed set properties of the matroid (given in Section~\ref{sec:matroid-theory}). These are maintained by ensuring that, for any two newly added sets $A,B \in \mathrm{F}[r+1]$, there exists $C \in \mathrm{F}[r]$ such that $A \cap B \subseteq C$. Until this point, this has been done by checking if the intersection of each such $A,B$ is contained in a set $C$ of rank $r$. We remember that one of the properties of the closed sets of a matroid is that the intersection of two closed sets is itself a closed set. Therefore, we do not need to find a closed set $C$ that \textit{contains} $A \cap B$, since if $A$ and $B$ are indeed closed sets, their intersection will be \textit{equal} to some closed set $C$ of lesser rank. This insight leads us to the next improvement: if we keep track of all added closed sets in a rank table, then we can memoize \pr{Superpose!} and replace the innermost loop with a constant time dictionary lookup.

\begin{jllisting}
  # The rank table maps from the representation of a set to its assigned rank.
  rank = Dict{T, UInt8}(0=>0)
  
  [...]
  
  # Superpose.
  push!(F, Set()) # Add F[r+1].
  while length(to_insert) > 0
    A = pop!(to_insert)
    push!(F[r+1], A)
    rank[A] = r
  
    
    for B in setdiff(F[r+1], A)
      if !haskey(rank, A&B) || rank[A&B] >= r
        # Update insert queue.
        push!(to_insert, A | B)
    
        # Update F[r+1].
        setdiff!(F[r+1], [A, B])
        push!(F[r+1], A | B)
    
        # Update rank table.
        rank[A|B] = r
        break
      end
    end
  end
\end{jllisting}
\begin{table*}[ht!]
  \centering
  \caption{Performance of $\texttt{random\_kmc\_v5}$.}
  \label{tab:perf_v5}
  \begin{threeparttable}
    \begin{tabular}{llllllllll}
      \toprule
      $n$ & $(p_1, p_2, \ldots)$ & Trials & Time  & GC Time & Bytes allocated \\
      \midrule
      10 & [0, 6, 0] & 100 & 0.0001335 & 0.0 & 138.966 KiB \\
      10 & [0, 5, 1] & 100 & 0.0001436 & 0.0 & 158.691 KiB \\
      10 & [0, 5, 2] & 100 & 0.0001928 & 0.0 & 167.487 KiB \\
      10 & [0, 6, 1] & 100 & 0.0002204 & 0.0 & 148.812 KiB \\
      10 & [0, 4, 2] & 100 & 0.0001578 & 0.0 & 173.455 KiB \\
      10 & [0, 3, 3] & 100 & 0.0001743 & 0.0 & 202.566 KiB \\
      10 & [0, 0, 6] & 100 & 0.0003433 & 0.0 & 431.089 KiB \\
      10 & [0, 1, 1, 1] & 100 & 0.0004987 & 0.0 & 439.511 KiB \\
      13 & [0, 6, 0] & 100 & 0.0004776 & 0.0 & 422.431 KiB \\
      13 & [0, 6, 2] & 100 & 0.0003469 & 0.0 & 441.621 KiB \\
      16 & [0, 6, 0] & 100 & 0.0009073 & 0.0 & 1010.452 KiB \\
      16 & [0, 6, 1] & 100 & 0.0007939 & 0.0 & 997.022 KiB \\
      16 & [0, 0, 6] & 100 & 0.0066951 & 0.0 & 8.564 MiB \\
      20 & [0, 6, 0] & 100 & 0.0030797 & 0.0 & 4.042 MiB  \\
      20 & [0, 6, 2] & 10 & 0.0022849 & 0.0 & 4.547 MiB  \\
      32 & [0, 6, 2, 1] & 10 & 0.0269912 & 0.0 & 63.082 MiB  \\
      \bottomrule
    \end{tabular}
  \end{threeparttable}
\end{table*}
The full code for \jlinl{random_kmc_v5} can be found in Appendix~\ref{apx:randkmcv5}. Table~\ref{tab:perf_v5} shows that implementing a rank table was an extremely significant improvement. For smaller matroids, it is around 5-10x faster, however it is for larger matroids that it truly outshines its predecessors -- \jlinl{random_kmc_v5} is a whopping 13~000 times faster than \jlinl{random_kmc_v4} with $n=16, p=[0,0,6]$ as input.


\subsubsection{Non-redundant cover generation}
Up to this point, our cover generation routine has not taken into account that any two sets of rank $r$ will have at least one cover in common. To see this, consider a matroid-under-construction with $n=10$ where $A = \{1,2\}$ and $B = \{1,3\}$ are closed sets of rank 2. Currently, \pr{Generate-Covers} will happily generate the cover $C=\{1,2,3\}$ twice, once as the cover of $A$ and subsequently as the cover of $B$. Throughout this analysis, we will assume the worst case scenario of no enlargements, as any enlargements will strictly lower the number of sets in play at a given rank. In this case, $|\mathrm{F}[r]| = \binom{n}{r}$, and for each closed set $A$ of rank $r$ we are generating $|E\setminus A| = (n-r)$ covers, giving us a total of $\binom{n}{r}(n-r)$ covers generated at each rank $r$, including the duplicates. With no enlargements, we know that there are $\binom{n}{r+1}$ covers, and

$$\begin{aligned}
  (n-r)\binom{n}{r} &= \frac{n!(n-r)}{r!(n-r)!} \\
                    &= \frac{n!}{r!(n-r-1)!} \\
                    &= (r+1)\frac{n!}{(r+1)!(n-r-1)!} \\
                    &= (r+1)\binom{n}{r+1}. \\
\end{aligned}$$
For each step $r$, we are generating $r+1$ times as many covers as we need to. Over the course of all steps $0\leq r\leq n$, we are generating $$\sum_{r=0}^n (r+1) = \sum_{r=1}^{n+1}r = T_{n+1}$$ times the actual number of covers, where $T_{n+1}=\frac{(n+1)(n+2)}{2}$ is the triangular number. In other words, if we find a way to generate each cover only once, we will have shaved off an $n^2$ factor from the asymptotic complexity of our implementation.

When generating covers, \jlinl{random_kmc_v6} improves upon the brute force cover generation described above by only adding the covers 
$$\Bigl\{ A \cup \{ a \} : A \in \mathrm{F}[r], a \in E \setminus A, a \notin \bigcup \bigl\{ B : B \in \mathrm{F}[r+1], A \subseteq B \bigr\} \Bigr\}.$$
In other words, we find the covers of $A$, that is, the sets obtained by adding one more element $a$ from $E$ to $A$, but we do not include any $a$ that is to be found in another, already added, cover $B$ that contains $A$. This solves the problem described above; the cover $\{1,2,3\} = B \cup \{ 2 \}$ will not be generated, as $2 \in C$ and $B \subseteq C$. This is implemented in the following manner:

\begin{jllisting}
  # Generate minimal closed sets for rank r+1 (random_kmc_v6)
  for y in F[r] # y is a closed set of rank r.
    t = E - y # The set of elements not in y.
    # Find all sets in F[r+1] that already contain y and remove excess elements from t.
    for x in F[r+1]
      if (x & y == y) t &= ~x end
      if t == 0 break end
    end
    # Insert y ∪ a for all a ∈ t.
    while t > 0
      x = y|(t&-t)
      add_set!(x, F, r, rank)
      t &= ~x
    end
  end
\end{jllisting}
We have extracted the iterative superpose logic described above into its own function to allow it to be performed on a cover-per-cover basis:
\begin{jllisting}
function add_set!(x, F, r, rank)
  if x in F[r+1] return end
  for y in F[r+1]
    if haskey(rank, x&y) && rank[x&y]<r
    continue
    end
    
    # x ∩ y has rank > r, replace with x ∪ y.
    setdiff!(F[r+1], y)
    return add_set!(x|y, F, r, rank)
  end
  
  push!(F[r+1], x)
  rank[x] = r
end
\end{jllisting}
As such, $\mathrm{F}[r+1]$ is empty when the first cover $y \in \mathrm{F}$ is generated, and all covers $\{y \cup \{a\} : a \in E \setminus y\}$ are added. For later sets $y$, we are comparing with the previously added covers, and dropping any element to be found in a cover $x$ that fully includes $y$. This way, we avoid re-generating the cover $x$.


The full code for \jlinl{random_kmc_v6} can be found in Appendix~\ref{apx:randkmcv6}.

\begin{table*}[ht!]
  \centering
  \caption{Performance of $\texttt{random\_kmc\_v6}$.}
  \label{tab:perf_v6}
  \begin{threeparttable}
    \begin{tabular}{llllllllll}
      \toprule
      $n$ & $(p_1, p_2, \ldots)$ & Trials & Time  & GC Time & Bytes allocated \\
      \midrule
      10 & [0, 6, 0] & 100 & 0.000157 & 0.0 & 11.306 KiB \\
      10 & [0, 5, 1] & 100 & 0.0001427 & 0.0 & 12.257 KiB \\
      10 & [0, 5, 2] & 100 & 0.000121 & 0.0 & 11.568 KiB \\
      10 & [0, 6, 1] & 100 & 8.61e-5 & 0.0 & 10.447 KiB \\
      10 & [0, 4, 2] & 100 & 0.0001237 & 0.0 & 13.597 KiB \\
      10 & [0, 3, 3] & 100 & 0.0001233 & 0.0 & 14.029 KiB \\
      10 & [0, 0, 6] & 100 & 0.0002856 & 0.0 & 15.414 KiB \\
      10 & [0, 1, 1, 1] & 100 & 0.0001942 & 0.0 & 14.446 KiB \\
      13 & [0, 6, 0] & 100 & 0.0004483 & 0.0 & 19.117 KiB \\
      13 & [0, 6, 2] & 100 & 0.0004541 & 0.0 & 18.957 KiB \\
      16 & [0, 6, 0] & 10 & 0.0014919 & 0.0 & 34.531 KiB \\
      16 & [0, 6, 1] & 10 & 0.0014731 & 0.0 & 36.016 KiB \\
      16 & [0, 0, 6] & 10 & 0.0168858 & 0.0 & 127.652 KiB \\
      20 & [0, 6, 0] & 10 & 0.0061574 & 0.0 & 81.573 KiB \\
      20 & [0, 6, 2] & 10 & 0.0059717 & 0.0 & 82.323 KiB \\
      32 & [0, 6, 2, 1] & 10 & 0.1599507 & 0.0 & 279.531 KiB \\
      63 & [0, 6, 4, 2, 1] & 1 & 11.138914 & 0.0 & 4.912 MiB \\
      64 & [0, 6, 4, 4, 2, 1] & 1 & 12.508729 & 0.0 & 4.912 MiB \\
      128 & [0, 6, 6, 4, 4, 2, 1] & 1 & 1232.8570 & 0.0114583 & 102.159 MiB \\
      \bottomrule
    \end{tabular}
  \end{threeparttable}
\end{table*}
\skelpar

\subsection{What do the generated matroids look like?}
\subsubsection{Observations}
\begin{enumerate}
  \item The average cardinality of the closed sets of a given rank is usually not very much higher than the rank. If the average cardinality were to stray much, all the sets merge instead and the sole closed set of that rank would become $E$. This might be a useful heuristic when finding the rank of a set.
\end{enumerate}

\skelpar{@Benchmarking. Histogrammer. Beskrive variansen i matroide-størrelse ifht input.}

\subsection{Producing the rank function and independent sets}
To build a general matroid library, we want to be able to access all properties of a generated matroid $\mathfrak{M}$. This would include:
\begin{enumerate}
  \item the bases $\mathcal{B}$ of $\mathfrak{M}$,
  \item the independent sets $\mathcal{I}$ of $\mathfrak{M}$,
  \item the circuits $\mathcal{C}$ of $\mathfrak{M}$,
  \item the closure function $\fn{cl} : 2^E \to \mathcal{F}$, and
  \item the rank function $\fn{rank} : 2^E \to \mathbb{Z}^*$ of $\mathfrak{M}$.
\end{enumerate}

In this section, I will first describe an extension of \pr{Knuth-Matroid} that is also fully enumerates $\mathcal{I}$ and $\mathcal{C}$ for $\mathfrak{M}$ when $n$ is small enough. However, this approach does not scale well for larger values of $n$. For values of $n$ up to 128, we will therefore restrict our attention to independent sets and the rank function, as these are the matroid properties that are relevant to our usecase of fair allocation.

\subsubsection{Finding circuits and independent sets for smaller matroids}
\cite{knuth-1975} includes an ALGOL W~\cite{wirth-1966} implementation that also generates the circuits and independent sets for the generated matroid. A later implementation in C called ERECTION.W can be found at~\cite{knuth-2003}. \jlinl{random_erection} is an extension of \jlinl{random_kmc_v6} that finds $\mathcal{I}$ and $\mathcal{C}$ by pre-populating the rank table with all subsets of $E$. The full source code for \jlinl{random_erection} can be found in Appendix~XXX.

\begin{jllisting}
  # Populate rank table with 100+cardinality for all subsets of E.
  k=1; rank[0]=100;
  while (k<=mask)
    for i in 0:k-1 rank[k+i] = rank[i]+1 end
    k=k+k;
  end
\end{jllisting}

Covers are generated and sets inserted in the same manner as in\linebreak\jlinl{random_kmc_v6}. After all covers and enlargements have been inserted and superposed (meaning $\mathrm{F}[r+1]$ contains the closed sets of rank $r+1$), a new operation, \jlinl{mark_independent_subsets!} is called on each closed set.

\skelpars[4]